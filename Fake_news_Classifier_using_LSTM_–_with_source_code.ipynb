{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Step 1 – Importing libraries required for Fake news Classifier.\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "OqyDJ2mWaj9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 – Reading input data."
      ],
      "metadata": {
        "id": "76M44coebKIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv.zip')\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(inplace=True)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "R2QpCkcIapAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 – Creating X and y data."
      ],
      "metadata": {
        "id": "TdkACoAHhKG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['title']\n",
        "y = df['label']"
      ],
      "metadata": {
        "id": "4ybU_-4wayWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For X we are just taking the title column. For y we are just taking the label column."
      ],
      "metadata": {
        "id": "1jxMgekVhUgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 – Cleaning input data."
      ],
      "metadata": {
        "id": "3lPA3ikgbXHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(X)):\n",
        "    text = X[i]\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "    text = [ps.stem(t) for t in text if t not in stopwords.words('english')]\n",
        "    corpus.append(' '.join(text))"
      ],
      "metadata": {
        "id": "hUljhizna1kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are traversing in X and then just simply using regex to clean our data and store it in the corpus list.\n",
        "\n",
        "First of all, we are just replacing everything that is not an alphabet with a space.\n",
        "\n",
        "Then we are lowercasing it and splitting it.\n",
        "\n",
        "Then we are checking if the words are not in stopwords, then stem it.\n",
        "\n",
        "Simply join these results and make a sentence out of them and add it to the corpus list."
      ],
      "metadata": {
        "id": "ei8t0elJbbvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 – Encoding input data."
      ],
      "metadata": {
        "id": "JSxERnWKbiql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000\n",
        "sent_len = 20\n",
        "\n",
        "one_hot_encoded = [one_hot(x,vocab_size) for x in corpus]\n",
        "one_hot_encoded = pad_sequences(one_hot_encoded,maxlen=sent_len)\n",
        "one_hot_encoded[0]"
      ],
      "metadata": {
        "id": "GBRAZuzja1qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are encoding our text data to numerical data using one_hot.\n",
        "Remember this one hot is not that 0s and 1s. In this one-hot encoding, we assign a random number using hashing to the word. The random word is chosen from the range 0-vocab_size.\n",
        "Then we are padding the sequences with 0s to make every line of the same length.\n",
        "And then simply we are checking how our first sentence looks like after these 2 operations."
      ],
      "metadata": {
        "id": "bj-zzQ-Bbr5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 – Processing X and y data."
      ],
      "metadata": {
        "id": "DH-CmEN1bxkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(one_hot_encoded)\n",
        "y = np.array(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "kCDir6Wrb4Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting X and y to NumPy arrays and simply splitting the data using traintestsplit."
      ],
      "metadata": {
        "id": "7wgjpx09b7G2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7 – Creating the model."
      ],
      "metadata": {
        "id": "xdz5Hnosb-vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_output_features = 40\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size,no_of_output_features,input_length=sent_len))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "FPcsLfJ7cBof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are creating our model.\n",
        "Our model has just 4 layers.\n",
        "The first layer is the Embedding layer which will convert the number array which we saw above into a vector of 40 dimensions followed by a Dropout layer.\n",
        "And then we have an LSTM layer with 100 nodes followed by a Dropout layer.\n",
        "Dropout layers are for preventing Overfitting."
      ],
      "metadata": {
        "id": "KhmDoLDHcIGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_output_features = 40\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,no_of_output_features,input_length=sent_len))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-5O7G1y1cLSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8 – Training the Fake news Classifier model."
      ],
      "metadata": {
        "id": "xieyK7VfcPmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=64,epochs=40)"
      ],
      "metadata": {
        "id": "VRAVUEl9cS5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9 –  Checking metrics of the model.Checking the accuracy of the Fake news Classifier model."
      ],
      "metadata": {
        "id": "L1qP6v9LgxXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "id": "Pcfek6E1fGij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}